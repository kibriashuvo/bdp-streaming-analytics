
I want to compute max tips per 15 mins computed in every 5 mins, keyed by location id

When tip was maximum customer behavior



Incremental Aggegation to be more efficient, rather than buffering up events in a window

## Part 1: Design for Streaming Analytics


**Q1:** *Select a dataset suitable for streaming analytics for a customer as a running example (thus the basic unit of the data should be a discrete record/event data). Explain the dataset and at least two different analytics for the customer: (i) a streaming analytics which analyzes streaming data from the customer (customerstreamapp) and (ii) a batch analytics which analyzes historical results outputted by the streaming analytics.* 

**Answer:** 

I used the New York City (NYC) Taxi [dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) published by NYC Taxi and Limousine Corporation (TLC). 

Particularly, I have used the dataset for yellow taxi cabs for the month *January, 2019*. The data is available in CSV format, where each row of the CSV file represents a taxi ride event in NYC. 
The dataset that I have used here contains in total 18 fields of which we are mostly interested in the following fields - 

* *tpep_pickup_datetime* - When the meter is started
* *DOLocationID* - Dropoff location ID
* *tip_amount* - The amount of tip given by the passenger by credit card payment

(Note: A list of the explanation of each individal fields can be found [here](https://data.cityofnewyork.us/api/views/t29m-gskq/files/89042b9b-8280-4339-bda2-d68f428a7499?download=true&filename=data_dictionary_trip_records_yellow.pdf))

Previously, this dataset used to include the latitude and longitude of the pickup and dropoff locations, but in the latest versions they have removed these attributes. Instead they are providing locationID (1 through 263) from the pickup and dropoff locations as they have divided the whole NYC into 263 locations. However, a [geoJSON format](https://geo.nyu.edu/catalog/nyu-2451-36743) of the 263 zones can be found in the Spatial Data Repository of **New York University (NYU)**

**Streaming analytics:**

The taxi drivers might be interested in finding the areas where customers usually gives more *tips*. So, here I am going to develop a streaming analytics for the taxi drivers (my customers), where they will be able to see the status quo of total tip amount paid by the passengers for the rides which started in these pickup location. So that, if they want they can go to such places in the hope of earning more. 

To be more clear, the taxi drivers will be able see the list of pickup locations updating in realtime .....

**Batch analytics from the historical output of Streaming analytics:**

Time of the day, for which the trips resulted into maximum tips. And as this analysis will be on the bounded data source generated by my streming analytics output, my customers will be able to see this batch analytics considering daily, weekly, monthly basis.


For the batch analytics part, the drivers can see the pickup locations which resulted into highest tips by hourly, daily, weekly or even monthly......



**Q2:** *Customers will send data through message brokers/messaging systems which become data stream sources. Discuss and explain the following aspects for the streaming analytics: (i) should the analytics handle keyed or non-keyed data streams for the customer data, and (ii) which types of delivery guarantees should be suitable.*

**Answer** 
i. The analytics should handle *keyed* data stream. Because, the analytics is going to calculate total amount tips of all the trips starting from a particular pickup location. So, to calculate this we must key the data stream by the pickup location id......


ii. *At most once* delivery guarantee should be suitable. Because, it we don't really want to process a single event multiple times and get a *false* high tip amount which might mislead the taxi drivers to choose wrong pick up locations.

On the other hand, the events that we are handling in this analytics *isn't something that actually requires At least once delivery guarantee*, because we want to provide the drivers an opportunity to earn more, but it is not something that will affect their normal earnings. Yes, they might earn more if we can provide the exact updated realtime analytics, but missing couple of events won't hurt them either.

...



**Q3:** *Given streaming data from the customer (selected before). Explain the following issues: (i) which types of time should be associated with stream sources for the analytics and be considered in stream processing (if the data sources have no timestamps associated with events, then what would be your solution), and (ii) which types of windows should be developed for the analytics (if no window, then why). Explain these aspects and give examples.* 


**Answer** 
i. The particular streaming analytics in our case doesn't require consideration of *event time*. Because, in my streaming analytics I am computing the total amount of tip per location. Here, my customer (taxi drivers) are mostly interested in knowing starting location of the trips which ends up resulting into maximum tips.
But as I'm also storing the output of my streaming analysis output to a sink `mysimbdp-coredms` for future batch processing, so I am considering *event time* during my streaming analytics. Because, then the drivers will be able to find out the starting times of the In the analytics that we are doing in this case requires considering *event-time????* during analytics. The reason behind this is, not only the drivers want to know the pickup locations 


ii. *Sliding windows* As our analytics will present the drivers with a regulargly updated status of the most rewarding locations. For, example my analytics will be computing max tips per 15 mins computed in every 5 mins. A new window will be created in every 5 mins, and each of this windows will contain the taxi ride events recieved in last 15 mins.....

overlap
Result materialization










    






